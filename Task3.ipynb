{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - Feature Matching using SIFT\n",
    "\n",
    "Write a function which takes an image from the same dataset for training\n",
    "and testing as in the previous task.\n",
    "\n",
    "**Main steps:**\n",
    "1. You first extract keypoints and feature descriptors from your\n",
    "Test and Train images using standard SIFT or SURF feature extraction\n",
    "function from a library.\n",
    "2. Then you match features between images which will give you the raw noisy matches (correspondences).\n",
    "3. Now you should decide which geometric transform to use to reject the outliers. (using RANSAC)\n",
    "4. Finally, you will\n",
    "define a score on the obtained inlier matches and will use this to detect the\n",
    "objects (icons) scoring high for a given Test image. A basic score is counting\n",
    "the inlier matches.\n",
    "\n",
    "**Output:**\n",
    "\n",
    "Detect objects in the Test images using SIFT or equivalent features (such as SURF), recognize to which class they belong, and identify\n",
    "their scales and orientations. Similar as Task2, for visual demonstration the\n",
    "function should open a box around each detected object and indicate its class\n",
    "label. This box is scaled and rotated according to the objectâ€™s scale and orientation. Demonstrate example images(s) of the outcome detection in your report. Besides, demonstrate example images(s) that shows the feature-based\n",
    "matches established between the recognised objects and a Test image, before\n",
    "and after the outlier refinement step.\n",
    "\n",
    "**Evaluation:**\n",
    "\n",
    "Evaluate your algorithm on all Test images to report the overall Intersection over Union (IoU), False Positive (FPR), True Positive (TPR) and\n",
    "Accuracy (ACC) rates, as well as the average runtime. Refer to the following report http://host.robots.ox.ac.uk/pascal/VOC/voc2012/devkit_\n",
    "doc.pdf section 4.4 for further information about the evaluation metrics.\n",
    "Show and explain cases where this scheme finds difficulty to perform correctly. Compare the SIFT/SURF results to that of Task2 algorithm e.g.,\n",
    "does it improve the overall speed or accuracy? How much? Why?\n",
    "\n",
    "**Hyperparameter tuning:**\n",
    "\n",
    "Similarly, you will have some hyper-parameters to tune. This includes the\n",
    "number of Octaves and the (within-octave) Scalelevels within SIFT to build\n",
    "scale-spaces for keypoint detection, and the MaxRatio parameter within the\n",
    "matchFeatures function to reject weak matches. How are these parameters\n",
    "set for this task? Show quantitatively why.\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "For task 2 and task 3, you are allowed to use library functions for creating the pyramid or using Gaussian convolution. You are also allowed to use the library functions for extracting features, for e.g. extracting SIFT features. You are allowed to use math libraries, for instance svd functions for computing the homography.\n",
    "\n",
    "You are *not* allowed to use the `cv2.matchTemplate` or `cv2.BFMatcher`.\n",
    "- Basically functions for matching features need to be coded. \n",
    "- You would need to implement RANSAC also yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import task3\n",
    "import cv2 as cv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "from pathlib import Path\n",
    "from task3 import ImageDataset, ObjectDetector, Verbosity\n",
    "\n",
    "QUERY_IMG_DIR = Path(\"IconDataset\", \"png\")\n",
    "TEST_IMG_DIR = Path(\"Task3Dataset\", \"images\")\n",
    "\n",
    "ANNOTATIONS_DIR = Path(\"Task3Dataset\", \"annotations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run detection pipeline on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_on_dataset(\n",
    "        test_imgs: ImageDataset, \n",
    "        query_imgs: ImageDataset, \n",
    "        sift_hps: Dict = {},\n",
    "        ransac_hps: Dict = {},\n",
    "        lowe_threshold: float = 0.7,\n",
    "        min_match_count: int = 10, \n",
    "        verbose: Verbosity = Verbosity.MEDIUM\n",
    "    ) -> Tuple[float, ...]:\n",
    "\n",
    "    acc_lst, tpr_list, fpr_lst, fnr_lst = [], [], [], []\n",
    "    detector = ObjectDetector(query_imgs, sift_hps, ransac_hps, verbose=False)\n",
    "\n",
    "    # Iterate through each test image and detect objects in it. Compare these detctions to the ground truth annotations.\n",
    "    for img, img_path in test_imgs:\n",
    "        annotations_path = ANNOTATIONS_DIR / img_path.with_suffix(\".csv\").name\n",
    "        img_annotations = pd.read_csv(annotations_path)\n",
    "\n",
    "        # print(f\"Detecting objects in {img_path.stem}\")\n",
    "        detections = detector.detect(img, lowe_threshold, min_match_count, draw=False)\n",
    "\n",
    "        acc, tpr, fpr, fnr = task3.evaluate_detections(detections, img_annotations)\n",
    "        # print(f\"ACC: {acc:.2f}, TPR: {tpr:.2f}, FPR: {fpr:.2f}, FNR: {fnr:.2f}\")\n",
    "        acc_lst.append(acc); tpr_list.append(tpr); fpr_lst.append(fpr); fnr_lst.append(fnr)\n",
    "\n",
    "    return np.mean(acc_lst), np.mean(tpr_list), np.mean(fpr_lst), np.mean(fnr_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimisation\n",
    "\n",
    "We'll use the following objective function to measure performance over a range of hyperparameters for `SIFT`, `RANSAC`, Lowe's Test, and minimum match counts. Then, using Bayesian optimisation, we'll minimise the function, hence we use '-accuracy'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2526bc4a64b64c529ac90d535f3fc362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 95\u001b[0m\n\u001b[1;32m     92\u001b[0m test_images \u001b[38;5;241m=\u001b[39m ImageDataset(TEST_IMG_DIR, file_ext\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     93\u001b[0m query_images \u001b[38;5;241m=\u001b[39m ImageDataset(QUERY_IMG_DIR, file_ext\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 95\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mgp_minimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjective_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdimensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mITERATIONS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m tqdm_o\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/projects/computer_vision/venv/lib/python3.10/site-packages/skopt/optimizer/gp.py:281\u001b[0m, in \u001b[0;36mgp_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, initial_point_generator, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, noise, n_jobs, model_queue_size, space_constraint)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m base_estimator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     base_estimator \u001b[38;5;241m=\u001b[39m cook_estimator(\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGP\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    276\u001b[0m         space\u001b[38;5;241m=\u001b[39mspace,\n\u001b[1;32m    277\u001b[0m         random_state\u001b[38;5;241m=\u001b[39mrng\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax),\n\u001b[1;32m    278\u001b[0m         noise\u001b[38;5;241m=\u001b[39mnoise,\n\u001b[1;32m    279\u001b[0m     )\n\u001b[0;32m--> 281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbase_minimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43macq_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macq_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkappa\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkappa\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43macq_optimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macq_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_random_starts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_random_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_initial_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_initial_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_point_generator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_point_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_restarts_optimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_restarts_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43my0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspace_constraint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspace_constraint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/computer_vision/venv/lib/python3.10/site-packages/skopt/optimizer/base.py:332\u001b[0m, in \u001b[0;36mbase_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, initial_point_generator, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs, model_queue_size, space_constraint)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_calls):\n\u001b[1;32m    331\u001b[0m     next_x \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mask()\n\u001b[0;32m--> 332\u001b[0m     next_y \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m     result \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mtell(next_x, next_y)\n\u001b[1;32m    334\u001b[0m     result\u001b[38;5;241m.\u001b[39mspecs \u001b[38;5;241m=\u001b[39m specs\n",
      "File \u001b[0;32m~/projects/computer_vision/venv/lib/python3.10/site-packages/skopt/utils.py:779\u001b[0m, in \u001b[0;36muse_named_args.<locals>.decorator.<locals>.wrapper\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    776\u001b[0m arg_dict \u001b[38;5;241m=\u001b[39m {dim\u001b[38;5;241m.\u001b[39mname: value \u001b[38;5;28;01mfor\u001b[39;00m dim, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(dimensions, x)}\n\u001b[1;32m    778\u001b[0m \u001b[38;5;66;03m# Call the wrapped objective function with the named arguments.\u001b[39;00m\n\u001b[0;32m--> 779\u001b[0m objective_value \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marg_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m objective_value\n",
      "Cell \u001b[0;32mIn[6], line 76\u001b[0m, in \u001b[0;36mobjective_function\u001b[0;34m(**params)\u001b[0m\n\u001b[1;32m     63\u001b[0m sift_hps \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m: params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msift_n_features\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnOctaveLayers\u001b[39m\u001b[38;5;124m'\u001b[39m: params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msift_n_octave_layers\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigma\u001b[39m\u001b[38;5;124m'\u001b[39m: params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msift_sigma\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     69\u001b[0m }\n\u001b[1;32m     71\u001b[0m ransac_hps \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_iterations\u001b[39m\u001b[38;5;124m'\u001b[39m: params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mransac_max_iterations\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthreshold\u001b[39m\u001b[38;5;124m'\u001b[39m: params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mransac_threshold\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     74\u001b[0m }\n\u001b[0;32m---> 76\u001b[0m acc, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_on_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msift_hps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mransac_hps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlowe_threshold\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin_match_count\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Negative because we minimise in the optimisation procedure.\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39macc\n",
      "Cell \u001b[0;32mIn[2], line 20\u001b[0m, in \u001b[0;36mdetect_on_dataset\u001b[0;34m(test_imgs, query_imgs, sift_hps, ransac_hps, lowe_threshold, min_match_count, verbose)\u001b[0m\n\u001b[1;32m     17\u001b[0m img_annotations \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(annotations_path)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# print(f\"Detecting objects in {img_path.stem}\")\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m detections \u001b[38;5;241m=\u001b[39m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlowe_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_match_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdraw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m acc, tpr, fpr, fnr \u001b[38;5;241m=\u001b[39m task3\u001b[38;5;241m.\u001b[39mevaluate_detections(detections, img_annotations)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# print(f\"ACC: {acc:.2f}, TPR: {tpr:.2f}, FPR: {fpr:.2f}, FNR: {fnr:.2f}\")\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/computer_vision/task3.py:367\u001b[0m, in \u001b[0;36mObjectDetector.detect\u001b[0;34m(self, train_img, lowe_ratio_test_threshold, min_match_count, draw)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;66;03m# For each descriptor in `desc_query`, find the `k` closest descriptors in \u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;66;03m# `desc_train`. We choose `k=2` for use in applying Lowe's ratio test, which\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;66;03m# is a method to filter out poor matches (outliers?). Lowe's ratio test is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;66;03m# many false matches (FPs) where the difference between the best and second-\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;66;03m# best is not significant.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m noisy_matches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatcher\u001b[38;5;241m.\u001b[39mknn_match(desc_query, desc_train, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 367\u001b[0m good_matches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_perform_lowes_ratio_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy_matches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlowe_ratio_test_threshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# We require at least `min_match_count` good matches to be present to consider\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# this query image present in our train image.\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(good_matches) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m min_match_count:\n",
      "File \u001b[0;32m~/projects/computer_vision/task3.py:433\u001b[0m, in \u001b[0;36mObjectDetector._perform_lowes_ratio_test\u001b[0;34m(self, matches, lowe_ratio_test_threshold)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_perform_lowes_ratio_test\u001b[39m(\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m    424\u001b[0m         matches: List[Tuple[DMatch, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]],\n\u001b[1;32m    425\u001b[0m         lowe_ratio_test_threshold: \u001b[38;5;28mfloat\u001b[39m\n\u001b[1;32m    426\u001b[0m         ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[DMatch]:\n\u001b[1;32m    427\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;124;03m    Filters out noisy, ambiguous matches using Lowe's ratio test.\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \n\u001b[1;32m    430\u001b[0m \u001b[38;5;124;03m    When writing the report, maybe we should discuss the empirical trade-off between\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;124;03m    false positives & true negatives and how varying this threshold affects them.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [ \n\u001b[1;32m    434\u001b[0m         first\n\u001b[1;32m    435\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m first, second \u001b[38;5;129;01min\u001b[39;00m matches\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m first\u001b[38;5;241m.\u001b[39mdistance \u001b[38;5;241m/\u001b[39m second\u001b[38;5;241m.\u001b[39mdistance \u001b[38;5;241m<\u001b[39m lowe_ratio_test_threshold\n\u001b[1;32m    437\u001b[0m     ]\n",
      "File \u001b[0;32m~/projects/computer_vision/task3.py:436\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_perform_lowes_ratio_test\u001b[39m(\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m    424\u001b[0m         matches: List[Tuple[DMatch, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]],\n\u001b[1;32m    425\u001b[0m         lowe_ratio_test_threshold: \u001b[38;5;28mfloat\u001b[39m\n\u001b[1;32m    426\u001b[0m         ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[DMatch]:\n\u001b[1;32m    427\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;124;03m    Filters out noisy, ambiguous matches using Lowe's ratio test.\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \n\u001b[1;32m    430\u001b[0m \u001b[38;5;124;03m    When writing the report, maybe we should discuss the empirical trade-off between\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;124;03m    false positives & true negatives and how varying this threshold affects them.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [ \n\u001b[1;32m    434\u001b[0m         first\n\u001b[1;32m    435\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m first, second \u001b[38;5;129;01min\u001b[39;00m matches\n\u001b[0;32m--> 436\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mfirst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistance\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msecond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistance\u001b[49m \u001b[38;5;241m<\u001b[39m lowe_ratio_test_threshold\n\u001b[1;32m    437\u001b[0m     ]\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Integer, Real\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.callbacks import Callable\n",
    "\n",
    "\n",
    "def convert_numpy(obj):\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()  # Convert arrays to lists\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "class LogDisplayProgressCallback(Callable):\n",
    "\n",
    "    def __init__(self, tqdm_obj):\n",
    "        self.tqdm_obj = tqdm_obj\n",
    "        self.iteration = 1\n",
    "\n",
    "    def __call__(self, result):\n",
    "        self.tqdm_obj.update(1)\n",
    "\n",
    "        # Since we are minimising negative accuracy.\n",
    "        best_score = -result.fun \n",
    "        current_params = [convert_numpy(x) for x in result.x]\n",
    "        \n",
    "        result_data = {\n",
    "            'iteration': self.iteration,\n",
    "            'best_score': best_score,\n",
    "            'parameters': dict(zip(\n",
    "                hyperparam_names, current_params\n",
    "            ))\n",
    "        }\n",
    "\n",
    "        with open('optimisation_log.json', 'a') as f:\n",
    "            json.dump(result_data, f)\n",
    "            f.write(',\\n')\n",
    "\n",
    "        self.iteration += 1\n",
    "\n",
    "\n",
    "# Define the hyperparameter space\n",
    "space = [\n",
    "    Integer(64, 128, name='sift_n_features'),\n",
    "    Integer(1, 10, name='sift_n_octave_layers'),\n",
    "    Real(0.01, 0.2, name='sift_contrast_threshold'),\n",
    "    Real(0.1, 20, name='sift_edge_threshold'),\n",
    "    Real(0.1, 3.0, name='sift_sigma'),\n",
    "    Real(0.5, 2.0, name='lowe_threshold'),\n",
    "    Integer(5, 50, name='min_match_count'),\n",
    "    Integer(14, 100, name='ransac_max_iterations'),\n",
    "    Real(1.0, 10.0, name='ransac_threshold'),\n",
    "]\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective_function(**params):\n",
    "    sift_hps = {\n",
    "        'nfeatures': params['sift_n_features'],\n",
    "        'nOctaveLayers': params['sift_n_octave_layers'],\n",
    "        'contrastThreshold': params['sift_contrast_threshold'],\n",
    "        'edgeThreshold': params['sift_edge_threshold'],\n",
    "        'sigma': params['sift_sigma'],\n",
    "    }\n",
    "\n",
    "    ransac_hps = {\n",
    "        'max_iterations': params['ransac_max_iterations'],\n",
    "        'threshold': params['ransac_threshold'],\n",
    "    }\n",
    "\n",
    "    acc, _, _, _ = detect_on_dataset(test_images, query_images, sift_hps, ransac_hps,\n",
    "                                     params['lowe_threshold'],\n",
    "                                     params['min_match_count'])\n",
    "    \n",
    "    # Negative because we minimise in the optimisation procedure.\n",
    "    return -acc\n",
    "\n",
    "\n",
    "ITERATIONS = 50\n",
    "hyperparam_names = [\n",
    "    param.name for param in space\n",
    "]\n",
    "\n",
    "tqdm_o = tqdm(total=ITERATIONS)\n",
    "callback = LogDisplayProgressCallback(tqdm_o)\n",
    "\n",
    "test_images = ImageDataset(TEST_IMG_DIR, file_ext=\"png\")\n",
    "query_images = ImageDataset(QUERY_IMG_DIR, file_ext=\"png\")\n",
    "\n",
    "result = gp_minimize(\n",
    "    objective_function,\n",
    "    dimensions=space,\n",
    "    n_calls=ITERATIONS,\n",
    "    random_state=42,\n",
    "    callback=[callback]\n",
    ")\n",
    "\n",
    "tqdm_o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
